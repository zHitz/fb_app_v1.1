# Facebook Post Scraper - Multi-Thread Edition

PhiÃªn báº£n multi-threading cá»§a Facebook Post Scraper vá»›i kháº£ nÄƒng xá»­ lÃ½ nhiá»u bÃ i viáº¿t cÃ¹ng lÃºc, tÄƒng tá»‘c Ä‘á»™ scraping Ä‘Ã¡ng ká»ƒ.

## âœ¨ TÃ­nh nÄƒng chÃ­nh

### ğŸš€ Multi-Threading
- **Concurrent Processing**: Xá»­ lÃ½ nhiá»u URL cÃ¹ng lÃºc
- **Configurable Thread Limit**: Giá»›i háº¡n sá»‘ thread (máº·c Ä‘á»‹nh 3-5)
- **Driver Pool Management**: Quáº£n lÃ½ pool cÃ¡c WebDriver instances
- **Rate Limiting**: TrÃ¡nh bá»‹ block bá»Ÿi Facebook

### ğŸ“Š Progress Tracking
- **Real-time Progress**: Hiá»ƒn thá»‹ tiáº¿n Ä‘á»™ real-time
- **Success Rate Monitoring**: Theo dÃµi tá»· lá»‡ thÃ nh cÃ´ng
- **Time Estimation**: Æ¯á»›c tÃ­nh thá»i gian cÃ²n láº¡i
- **Error Handling**: Xá»­ lÃ½ lá»—i comprehensive

### ğŸ› ï¸ Advanced Features
- **Retry Logic**: Thá»­ láº¡i cÃ¡c request tháº¥t báº¡i
- **Resource Management**: Tá»± Ä‘á»™ng cleanup resources
- **Multiple Output Formats**: TXT, CSV, JSON
- **Settings Configuration**: Cáº¥u hÃ¬nh linh hoáº¡t

## ğŸ“ Cáº¥u trÃºc File

```
fb_app/
â”œâ”€â”€ driver_pool.py           # Quáº£n lÃ½ pool WebDriver
â”œâ”€â”€ multi_thread_scraper.py  # Core multi-threading logic
â”œâ”€â”€ app_multi.py             # GUI app vá»›i multi-threading
â”œâ”€â”€ index_multi.html         # UI cho multi-threading
â”œâ”€â”€ run_multi.py             # Command-line runner
â”œâ”€â”€ facebook_scrapper.py     # Original scraper logic
â””â”€â”€ requirements.txt         # Dependencies
```

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. GUI Application (Khuyáº¿n nghá»‹)

```bash
# Cháº¡y á»©ng dá»¥ng GUI vá»›i multi-threading
python app_multi.py
```

**TÃ­nh nÄƒng GUI:**
- âœ… Real-time progress tracking
- âœ… Settings configuration
- âœ… Stop/Resume functionality
- âœ… Multiple save formats
- âœ… Dark/Light theme
- âœ… Error reporting

### 2. Command Line

```bash
# Cháº¡y tá»« command line vá»›i 3 workers (máº·c Ä‘á»‹nh)
python run_multi.py

# Cháº¡y vá»›i sá»‘ workers tÃ¹y chá»‰nh
python run_multi.py 5
```

### 3. Programmatic Usage

```python
from multi_thread_scraper import MultiThreadScraper, create_progress_callback

# URLs to scrape
urls = [
    "https://facebook.com/post1",
    "https://facebook.com/post2",
    "https://facebook.com/post3"
]

# Create progress callback
progress_callback = create_progress_callback()

# Initialize scraper
scraper = MultiThreadScraper(
    max_workers=3,
    driver_pool_size=3,
    rate_limit_delay=(2, 5),
    max_retries=3,
    progress_callback=progress_callback
)

try:
    # Perform scraping
    results = scraper.scrape_urls(urls)
    
    # Save results
    scraper.save_results(results, "txt")
    
finally:
    # Cleanup
    scraper.shutdown()
```

## âš™ï¸ Cáº¥u hÃ¬nh

### CÃ i Ä‘áº·t cÆ¡ báº£n

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|--------|
| `max_workers` | 3 | Sá»‘ lÆ°á»£ng thread tá»‘i Ä‘a |
| `driver_pool_size` | 3 | Sá»‘ lÆ°á»£ng WebDriver instances |
| `rate_limit_delay` | (2, 5) | Delay giá»¯a cÃ¡c request (giÃ¢y) |
| `max_retries` | 3 | Sá»‘ láº§n thá»­ láº¡i khi tháº¥t báº¡i |

### CÃ i Ä‘áº·t nÃ¢ng cao

```python
# TÃ¹y chá»‰nh cáº¥u hÃ¬nh cho hiá»‡u suáº¥t tá»‘i Æ°u
config = {
    'max_workers': 5,           # TÄƒng sá»‘ workers
    'driver_pool_size': 5,      # TÄƒng pool size
    'rate_limit_min': 1,        # Giáº£m delay tá»‘i thiá»ƒu
    'rate_limit_max': 3,        # Giáº£m delay tá»‘i Ä‘a
    'max_retries': 2            # Giáº£m sá»‘ láº§n retry
}
```

## ğŸ“Š Performance Comparison

| Metric | Single Thread | Multi-Thread (3 workers) | Improvement |
|--------|---------------|---------------------------|-------------|
| **Speed** | 1x | 3-5x | 300-500% |
| **Resource Usage** | Low | Medium | Optimized |
| **Stability** | High | High | Maintained |
| **Error Recovery** | Basic | Advanced | Enhanced |

## ğŸ”§ Troubleshooting

### Common Issues

1. **"No drivers available in pool"**
   ```python
   # Giáº£i phÃ¡p: TÄƒng driver_pool_size hoáº·c giáº£m max_workers
   scraper = MultiThreadScraper(
       max_workers=2,
       driver_pool_size=3
   )
   ```

2. **High memory usage**
   ```python
   # Giáº£i phÃ¡p: Giáº£m sá»‘ workers vÃ  tÄƒng rate limit
   scraper = MultiThreadScraper(
       max_workers=2,
       rate_limit_delay=(3, 7)
   )
   ```

3. **Getting blocked by Facebook**
   ```python
   # Giáº£i phÃ¡p: TÄƒng rate limiting
   scraper = MultiThreadScraper(
       max_workers=2,
       rate_limit_delay=(5, 10)
   )
   ```

### Logging

Kiá»ƒm tra logs Ä‘á»ƒ debug:

```bash
# GUI app logs
tail -f scraper_multi.log

# Command line logs
tail -f scraper_multi.log
```

## ğŸ›¡ï¸ Best Practices

### 1. Tá»‘i Æ°u hiá»‡u suáº¥t
- Báº¯t Ä‘áº§u vá»›i 3 workers, tÄƒng dáº§n náº¿u cáº§n
- Monitor CPU vÃ  memory usage
- Sá»­ dá»¥ng rate limiting phÃ¹ há»£p

### 2. TrÃ¡nh bá»‹ block
- KhÃ´ng sá»­ dá»¥ng quÃ¡ nhiá»u workers (>5)
- TÄƒng delay giá»¯a requests náº¿u cáº§n
- Sá»­ dá»¥ng different user profiles náº¿u cÃ³ thá»ƒ

### 3. Error handling
- LuÃ´n sá»­ dá»¥ng try-catch khi gá»i API
- Implement proper cleanup
- Monitor logs for issues

## ğŸ“ˆ Monitoring & Analytics

### Progress Tracking
```python
def custom_progress_callback(progress, result=None):
    print(f"Progress: {progress['completed_tasks']}/{progress['total_tasks']}")
    print(f"Success Rate: {progress['success_rate']}%")
    print(f"Elapsed: {progress['elapsed_time']}s")
    
    if result:
        status = "âœ“" if result.success else "âœ—"
        print(f"  {status} {result.url} ({result.processing_time:.2f}s)")
```

### Driver Pool Status
```python
# Kiá»ƒm tra tráº¡ng thÃ¡i driver pool
status = scraper.get_driver_pool_status()
print(f"Available drivers: {status['available_drivers']}")
print(f"Active drivers: {status['active_drivers']}")
```

## ğŸ”„ Migration tá»« Single-Thread

Äá»ƒ migrate tá»« phiÃªn báº£n single-thread:

1. **Thay tháº¿ import:**
   ```python
   # CÅ©
   from facebook_scrapper import main
   
   # Má»›i
   from multi_thread_scraper import MultiThreadScraper
   ```

2. **Cáº­p nháº­t code:**
   ```python
   # CÅ©
   for url in urls:
       result = extract_data(driver, url)
   
   # Má»›i
   scraper = MultiThreadScraper()
   results = scraper.scrape_urls(urls)
   ```

3. **Backward compatibility:**
   ```python
   # API cÅ© váº«n hoáº¡t Ä‘á»™ng vá»›i app_multi.py
   api.scrape(single_url)  # Sá»­ dá»¥ng multi-threading internally
   ```

## ğŸ†˜ Support

Náº¿u gáº·p váº¥n Ä‘á»:

1. Kiá»ƒm tra logs: `scraper_multi.log`
2. Verify Chrome profile path trong `driver_pool.py`
3. Äáº£m báº£o Ä‘á»§ RAM cho multiple drivers
4. Giáº£m sá»‘ workers náº¿u system khÃ´ng á»•n Ä‘á»‹nh

## ğŸ“ Changelog

### v1.1 - Multi-Thread Edition
- âœ… Added multi-threading support
- âœ… Implemented driver pool management
- âœ… Added progress tracking
- âœ… Enhanced error handling
- âœ… Created new GUI interface
- âœ… Added command-line runner
- âœ… Improved resource management

---

**Note**: CÃ´ng cá»¥ nÃ y chá»‰ dÃ nh cho má»¥c Ä‘Ã­ch giÃ¡o dá»¥c. HÃ£y tuÃ¢n thá»§ Terms of Service cá»§a Facebook khi sá»­ dá»¥ng. 