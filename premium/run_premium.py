#!/usr/bin/env python3
"""
Premium Facebook Scraper - Command Line Runner
Advanced content extraction and media download
"""

import sys
import asyncio
import logging
from pathlib import Path

# Add parent directory to path for imports
sys.path.append('../fb_app')

from premium_scraper import PremiumScraper, create_premium_progress_callback
from data_structures import ScraperConfig

def read_urls_from_file(filename="links.txt"):
    """Read URLs from a text file."""
    urls = []
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            for line in file:
                cleaned_line = line.strip()
                if cleaned_line:
                    urls.append(cleaned_line)
        print(f"‚úì ƒê·ªçc ƒë∆∞·ª£c {len(urls)} URLs t·ª´ {filename}")
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file '{filename}'")
        return []
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
        return []
    return urls

def print_banner():
    """Print premium banner"""
    print("=" * 80)
    print("üöÄ FACEBOOK SCRAPER PREMIUM EDITION")
    print("   Advanced Content & Media Extraction")
    print("=" * 80)
    print()

def print_config(config: ScraperConfig):
    """Print current configuration"""
    print("‚öôÔ∏è  C·∫§U H√åNH PREMIUM:")
    print(f"   ‚Ä¢ Max Workers: {config.max_workers}")
    print(f"   ‚Ä¢ Driver Pool: {config.driver_pool_size}")
    print(f"   ‚Ä¢ Rate Limit: {config.rate_limit_min}-{config.rate_limit_max}s")
    print(f"   ‚Ä¢ Content Extraction: {'‚úì' if config.extract_content else '‚úó'}")
    print(f"   ‚Ä¢ Media Download: {'‚úì' if config.download_media else '‚úó'}")
    print(f"   ‚Ä¢ Media Quality: {config.media_quality}")
    print(f"   ‚Ä¢ Max File Size: {config.max_media_size_mb}MB")
    print(f"   ‚Ä¢ Create Thumbnails: {'‚úì' if config.create_thumbnails else '‚úó'}")
    print(f"   ‚Ä¢ Download Folder: {config.base_download_folder}")
    print()

async def main():
    """Main premium scraping function"""
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('premium_scraper.log'),
            logging.StreamHandler()
        ]
    )
    
    print_banner()
    
    # Read URLs
    urls = read_urls_from_file("links.txt")
    if not urls:
        print("‚ùå Kh√¥ng c√≥ URLs ƒë·ªÉ x·ª≠ l√Ω. Vui l√≤ng th√™m URLs v√†o links.txt")
        return
    
    # Configuration from command line arguments
    max_workers = 3
    extract_content = True
    download_media = True
    media_quality = "high"
    
    # Parse command line arguments
    if len(sys.argv) > 1:
        try:
            max_workers = int(sys.argv[1])
            print(f"üìä S·ª≠ d·ª•ng {max_workers} workers")
        except ValueError:
            print(f"‚ö†Ô∏è  Worker count kh√¥ng h·ª£p l·ªá: {sys.argv[1]}, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh: {max_workers}")
    
    if len(sys.argv) > 2:
        extract_content = sys.argv[2].lower() in ['true', '1', 'yes', 'on']
        print(f"üìù Content extraction: {'‚úì' if extract_content else '‚úó'}")
    
    if len(sys.argv) > 3:
        download_media = sys.argv[3].lower() in ['true', '1', 'yes', 'on']
        print(f"üíæ Media download: {'‚úì' if download_media else '‚úó'}")
    
    if len(sys.argv) > 4:
        media_quality = sys.argv[4].lower()
        if media_quality not in ['low', 'medium', 'high']:
            media_quality = 'high'
        print(f"üé• Media quality: {media_quality}")
    
    # Create premium configuration
    config = ScraperConfig(
        max_workers=max_workers,
        driver_pool_size=max_workers,
        rate_limit_min=2.0,
        rate_limit_max=5.0,
        max_retries=3,
        extract_content=extract_content,
        content_preview_words=50,
        extract_hashtags=True,
        extract_mentions=True,
        extract_links=True,
        download_media=download_media,
        media_quality=media_quality,
        max_media_size_mb=50,
        create_thumbnails=True,
        organize_by_date=True,
        base_download_folder="downloads"
    )
    
    print_config(config)
    
    # Create premium progress callback
    progress_callback = create_premium_progress_callback()
    
    # Initialize premium scraper
    scraper = PremiumScraper(config)
    scraper.set_progress_callback(progress_callback)
    
    try:
        print("üöÄ B·∫Øt ƒë·∫ßu Premium Scraping...")
        print("-" * 80)
        
        # Perform premium scraping
        results = await scraper.scrape_urls(urls)
        
        print("-" * 80)
        print("üéâ Premium Scraping ho√†n th√†nh!")
        print()
        
        # Detailed summary
        successful = sum(1 for r in results if r.success)
        failed = len(results) - successful
        total_content = sum(1 for r in results if r.success and r.post_data.content)
        total_media_found = sum(r.post_data.media_count for r in results if r.success)
        total_media_downloaded = sum(r.media_downloaded for r in results)
        total_download_size = sum(r.total_download_size for r in results)
        
        print("üìä T·ªîNG K·∫æT PREMIUM:")
        print(f"   ‚Ä¢ T·ªïng posts x·ª≠ l√Ω: {len(results)}")
        print(f"   ‚Ä¢ Th√†nh c√¥ng: {successful}")
        print(f"   ‚Ä¢ Th·∫•t b·∫°i: {failed}")
        print(f"   ‚Ä¢ T·ª∑ l·ªá th√†nh c√¥ng: {(successful/len(results)*100):.1f}%")
        print()
        
        if extract_content:
            print("üìù CONTENT EXTRACTION:")
            print(f"   ‚Ä¢ Posts c√≥ content: {total_content}")
            print(f"   ‚Ä¢ T·ª∑ l·ªá tr√≠ch xu·∫•t: {(total_content/max(successful,1)*100):.1f}%")
            print()
        
        if download_media:
            print("üíæ MEDIA DOWNLOAD:")
            print(f"   ‚Ä¢ Media t√¨m th·∫•y: {total_media_found}")
            print(f"   ‚Ä¢ Media ƒë√£ t·∫£i: {total_media_downloaded}")
            print(f"   ‚Ä¢ T·ª∑ l·ªá download: {(total_media_downloaded/max(total_media_found,1)*100):.1f}%")
            print(f"   ‚Ä¢ T·ªïng dung l∆∞·ª£ng: {total_download_size/1024/1024:.1f} MB")
            print()
        
        # Save results
        print("üíæ ƒêang l∆∞u k·∫øt qu·∫£...")
        
        # Create output directory
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        
        # Save in multiple formats
        timestamp = asyncio.get_event_loop().time()
        
        # JSON format (detailed)
        import json
        json_data = []
        for result in results:
            json_data.append({
                'url': result.url,
                'success': result.success,
                'user_name': result.post_data.user_name if result.success else None,
                'post_type': result.post_data.post_type.value if result.success and result.post_data.post_type else None,
                'content': {
                    'full_text': result.post_data.content.full_text if result.success and result.post_data.content else None,
                    'word_count': result.post_data.content.word_count if result.success and result.post_data.content else 0,
                    'hashtags': result.post_data.content.hashtags if result.success and result.post_data.content else [],
                    'mentions': result.post_data.content.mentions if result.success and result.post_data.content else [],
                } if result.success and result.post_data.content else None,
                'stats': {
                    'likes': result.post_data.stats.likes if result.success and result.post_data.stats else '0',
                    'comments': result.post_data.stats.comments if result.success and result.post_data.stats else '0',
                    'shares': result.post_data.stats.shares if result.success and result.post_data.stats else '0',
                } if result.success and result.post_data.stats else None,
                'media': {
                    'total_found': result.post_data.media_count if result.success else 0,
                    'downloaded': result.media_downloaded,
                    'failed': result.media_failed,
                    'total_size_mb': round(result.total_download_size / 1024 / 1024, 2)
                },
                'files': {
                    'local_folder': result.post_data.local_folder if result.success else None,
                    'content_file': result.post_data.content_file if result.success else None,
                    'metadata_file': result.post_data.metadata_file if result.success else None,
                },
                'processing_time': result.processing_time,
                'error_message': result.error_message
            })
        
        json_file = output_dir / f"premium_results_{int(timestamp)}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        # CSV format (summary)
        import csv
        csv_file = output_dir / f"premium_summary_{int(timestamp)}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['url', 'success', 'user_name', 'post_type', 'likes', 'comments', 'shares', 
                         'content_words', 'media_found', 'media_downloaded', 'download_size_mb', 
                         'local_folder', 'processing_time', 'error_message']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in results:
                writer.writerow({
                    'url': result.url,
                    'success': result.success,
                    'user_name': result.post_data.user_name if result.success else '',
                    'post_type': result.post_data.post_type.value if result.success and result.post_data.post_type else '',
                    'likes': result.post_data.stats.likes if result.success and result.post_data.stats else '0',
                    'comments': result.post_data.stats.comments if result.success and result.post_data.stats else '0',
                    'shares': result.post_data.stats.shares if result.success and result.post_data.stats else '0',
                    'content_words': result.post_data.content.word_count if result.success and result.post_data.content else 0,
                    'media_found': result.post_data.media_count if result.success else 0,
                    'media_downloaded': result.media_downloaded,
                    'download_size_mb': round(result.total_download_size / 1024 / 1024, 2),
                    'local_folder': result.post_data.local_folder if result.success else '',
                    'processing_time': round(result.processing_time, 2),
                    'error_message': result.error_message or ''
                })
        
        # TXT format (readable)
        txt_file = output_dir / f"premium_readable_{int(timestamp)}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write("üöÄ FACEBOOK SCRAPER PREMIUM - K·∫æT QU·∫¢\n")
            f.write("=" * 80 + "\n\n")
            
            for i, result in enumerate(results, 1):
                f.write(f"üìù POST #{i}\n")
                f.write("-" * 40 + "\n")
                f.write(f"URL: {result.url}\n")
                
                if result.success:
                    data = result.post_data
                    f.write(f"‚úì Th√†nh c√¥ng\n")
                    f.write(f"üë§ Ng∆∞·ªùi ƒëƒÉng: {data.user_name or 'Unknown'}\n")
                    f.write(f"üìä Lo·∫°i b√†i vi·∫øt: {data.post_type.value if data.post_type else 'unknown'}\n")
                    
                    if data.content:
                        f.write(f"üìù N·ªôi dung ({data.content.word_count} t·ª´):\n")
                        f.write(f"{data.content.full_text[:200]}{'...' if len(data.content.full_text) > 200 else ''}\n")
                        if data.content.hashtags:
                            f.write(f"üè∑Ô∏è  Hashtags: {', '.join(data.content.hashtags)}\n")
                        if data.content.mentions:
                            f.write(f"üë• Mentions: {', '.join(data.content.mentions)}\n")
                    
                    if data.stats:
                        f.write(f"‚ù§Ô∏è  {data.stats.likes} likes, üí¨ {data.stats.comments} comments, üîÑ {data.stats.shares} shares\n")
                    
                    if data.media_count > 0:
                        f.write(f"üñºÔ∏è  Media: {data.media_count} t√¨m th·∫•y, {result.media_downloaded} ƒë√£ t·∫£i\n")
                        if result.total_download_size > 0:
                            f.write(f"üíæ Dung l∆∞·ª£ng: {result.total_download_size/1024/1024:.1f} MB\n")
                        if data.local_folder:
                            f.write(f"üìÅ Th∆∞ m·ª•c: {data.local_folder}\n")
                    
                    f.write(f"‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {result.processing_time:.2f}s\n")
                else:
                    f.write(f"‚ùå Th·∫•t b·∫°i: {result.error_message}\n")
                
                f.write("\n")
        
        print(f"‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u:")
        print(f"   ‚Ä¢ JSON (chi ti·∫øt): {json_file}")
        print(f"   ‚Ä¢ CSV (t√≥m t·∫Øt): {csv_file}")
        print(f"   ‚Ä¢ TXT (d·ªÖ ƒë·ªçc): {txt_file}")
        
        if download_media and any(r.post_data.local_folder for r in results if r.success):
            print(f"   ‚Ä¢ Media files: downloads/ folder")
        
        # Show failed URLs if any
        if failed > 0:
            print()
            print("‚ùå C√ÅC URLs TH·∫§T B·∫†I:")
            for result in results:
                if not result.success:
                    print(f"   ‚Ä¢ {result.url}: {result.error_message}")
        
        print()
        print("üéâ Ho√†n th√†nh Premium Scraping!")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Ng∆∞·ªùi d√πng d·ª´ng scraping")
    except Exception as e:
        print(f"\n‚ùå L·ªói trong qu√° tr√¨nh scraping: {e}")
        logging.error(f"Unexpected error: {e}")
    finally:
        # Cleanup
        print("\nüßπ ƒêang d·ªçn d·∫πp...")
        scraper.shutdown()
        print("‚úÖ Ho√†n t·∫•t!")

def print_usage():
    """Print usage information"""
    print("üöÄ FACEBOOK SCRAPER PREMIUM - COMMAND LINE")
    print()
    print("Usage:")
    print("  python run_premium.py [workers] [extract_content] [download_media] [media_quality]")
    print()
    print("Arguments:")
    print("  workers         : S·ªë workers (1-10, m·∫∑c ƒë·ªãnh: 3)")
    print("  extract_content : true/false (m·∫∑c ƒë·ªãnh: true)")
    print("  download_media  : true/false (m·∫∑c ƒë·ªãnh: true)")
    print("  media_quality   : low/medium/high (m·∫∑c ƒë·ªãnh: high)")
    print()
    print("Examples:")
    print("  python run_premium.py                    # M·∫∑c ƒë·ªãnh")
    print("  python run_premium.py 5                  # 5 workers")
    print("  python run_premium.py 3 true false       # Kh√¥ng t·∫£i media")
    print("  python run_premium.py 2 true true medium # Ch·∫•t l∆∞·ª£ng trung b√¨nh")
    print()
    print("Requirements:")
    print("  ‚Ä¢ File links.txt ch·ª©a c√°c Facebook URLs")
    print("  ‚Ä¢ M·ªói URL tr√™n m·ªôt d√≤ng ri√™ng")
    print("  ‚Ä¢ Chrome browser ƒë√£ c√†i ƒë·∫∑t")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help', 'help']:
        print_usage()
    else:
        asyncio.run(main()) 