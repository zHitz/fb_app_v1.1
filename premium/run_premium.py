#!/usr/bin/env python3
"""
Premium Facebook Scraper - Command Line Runner
Advanced content extraction and media download
"""

import sys
import asyncio
import logging
from pathlib import Path

# Add parent directory to path for imports
sys.path.append('../fb_app')

from premium_scraper import PremiumScraper, create_premium_progress_callback
from data_structures import ScraperConfig

def read_urls_from_file(filename="links.txt"):
    """Read URLs from a text file."""
    urls = []
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            for line in file:
                cleaned_line = line.strip()
                if cleaned_line:
                    urls.append(cleaned_line)
        print(f"âœ“ Äá»c Ä‘Æ°á»£c {len(urls)} URLs tá»« {filename}")
    except FileNotFoundError:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file '{filename}'")
        return []
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file: {e}")
        return []
    return urls

def print_banner():
    """Print premium banner"""
    print("=" * 80)
    print("ðŸš€ FACEBOOK SCRAPER PREMIUM EDITION")
    print("   Advanced Content & Media Extraction")
    print("=" * 80)
    print()

def print_config(config: ScraperConfig):
    """Print current configuration"""
    print("âš™ï¸  Cáº¤U HÃŒNH PREMIUM:")
    print(f"   â€¢ Max Workers: {config.max_workers}")
    print(f"   â€¢ Driver Pool: {config.driver_pool_size}")
    print(f"   â€¢ Rate Limit: {config.rate_limit_min}-{config.rate_limit_max}s")
    print(f"   â€¢ Content Extraction: {'âœ“' if config.extract_content else 'âœ—'}")
    print(f"   â€¢ Media Download: {'âœ“' if config.download_media else 'âœ—'}")
    print(f"   â€¢ Media Quality: {config.media_quality}")
    print(f"   â€¢ Max File Size: {config.max_media_size_mb}MB")
    print(f"   â€¢ Create Thumbnails: {'âœ“' if config.create_thumbnails else 'âœ—'}")
    print(f"   â€¢ Download Folder: {config.base_download_folder}")
    print()

async def main():
    """Main premium scraping function"""
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('premium_scraper.log'),
            logging.StreamHandler()
        ]
    )
    
    print_banner()
    
    # Read URLs
    urls = read_urls_from_file("links.txt")
    if not urls:
        print("âŒ KhÃ´ng cÃ³ URLs Ä‘á»ƒ xá»­ lÃ½. Vui lÃ²ng thÃªm URLs vÃ o links.txt")
        return
    
    # Configuration from command line arguments
    max_workers = 3
    extract_content = True
    download_media = True
    media_quality = "high"
    
    # Parse command line arguments
    if len(sys.argv) > 1:
        try:
            max_workers = int(sys.argv[1])
            print(f"ðŸ“Š Sá»­ dá»¥ng {max_workers} workers")
        except ValueError:
            print(f"âš ï¸  Worker count khÃ´ng há»£p lá»‡: {sys.argv[1]}, sá»­ dá»¥ng máº·c Ä‘á»‹nh: {max_workers}")
    
    if len(sys.argv) > 2:
        extract_content = sys.argv[2].lower() in ['true', '1', 'yes', 'on']
        print(f"ðŸ“ Content extraction: {'âœ“' if extract_content else 'âœ—'}")
    
    if len(sys.argv) > 3:
        download_media = sys.argv[3].lower() in ['true', '1', 'yes', 'on']
        print(f"ðŸ’¾ Media download: {'âœ“' if download_media else 'âœ—'}")
    
    if len(sys.argv) > 4:
        media_quality = sys.argv[4].lower()
        if media_quality not in ['low', 'medium', 'high']:
            media_quality = 'high'
        print(f"ðŸŽ¥ Media quality: {media_quality}")
    
    # Create premium configuration
    config = ScraperConfig(
        max_workers=max_workers,
        driver_pool_size=max_workers,
        rate_limit_min=2.0,
        rate_limit_max=5.0,
        max_retries=3,
        extract_content=extract_content,
        content_preview_words=50,
        extract_hashtags=True,
        extract_mentions=True,
        extract_links=True,
        download_media=download_media,
        media_quality=media_quality,
        max_media_size_mb=50,
        create_thumbnails=True,
        organize_by_date=True,
        base_download_folder="downloads"
    )
    
    print_config(config)
    
    # Create premium progress callback
    progress_callback = create_premium_progress_callback()
    
    # Initialize premium scraper
    scraper = PremiumScraper(config)
    scraper.set_progress_callback(progress_callback)
    
    try:
        print("ðŸš€ Báº¯t Ä‘áº§u Premium Scraping...")
        print("-" * 80)
        
        # Perform premium scraping
        results = await scraper.scrape_urls(urls)
        
        print("-" * 80)
        print("ðŸŽ‰ Premium Scraping hoÃ n thÃ nh!")
        print()
        
        # Detailed summary
        successful = sum(1 for r in results if r.success)
        failed = len(results) - successful
        total_content = sum(1 for r in results if r.success and r.post_data.content)
        total_media_found = sum(r.post_data.media_count for r in results if r.success)
        total_media_downloaded = sum(r.media_downloaded for r in results)
        total_download_size = sum(r.total_download_size for r in results)
        
        print("ðŸ“Š Tá»”NG Káº¾T PREMIUM:")
        print(f"   â€¢ Tá»•ng posts xá»­ lÃ½: {len(results)}")
        print(f"   â€¢ ThÃ nh cÃ´ng: {successful}")
        print(f"   â€¢ Tháº¥t báº¡i: {failed}")
        print(f"   â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {(successful/len(results)*100):.1f}%")
        print()
        
        if extract_content:
            print("ðŸ“ CONTENT EXTRACTION:")
            print(f"   â€¢ Posts cÃ³ content: {total_content}")
            print(f"   â€¢ Tá»· lá»‡ trÃ­ch xuáº¥t: {(total_content/max(successful,1)*100):.1f}%")
            print()
        
        if download_media:
            print("ðŸ’¾ MEDIA DOWNLOAD:")
            print(f"   â€¢ Media tÃ¬m tháº¥y: {total_media_found}")
            print(f"   â€¢ Media Ä‘Ã£ táº£i: {total_media_downloaded}")
            print(f"   â€¢ Tá»· lá»‡ download: {(total_media_downloaded/max(total_media_found,1)*100):.1f}%")
            print(f"   â€¢ Tá»•ng dung lÆ°á»£ng: {total_download_size/1024/1024:.1f} MB")
            print()
        
        # Save results
        print("ðŸ’¾ Äang lÆ°u káº¿t quáº£...")
        
        # Create output directory
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        
        # Save in multiple formats
        timestamp = asyncio.get_event_loop().time()
        
        # JSON format (detailed)
        import json
        json_data = []
        for result in results:
            json_data.append({
                'url': result.url,
                'success': result.success,
                'user_name': result.post_data.user_name if result.success else None,
                'post_type': result.post_data.post_type.value if result.success and result.post_data.post_type else None,
                'content': {
                    'full_text': result.post_data.content.full_text if result.success and result.post_data.content else None,
                    'word_count': result.post_data.content.word_count if result.success and result.post_data.content else 0,
                    'hashtags': result.post_data.content.hashtags if result.success and result.post_data.content else [],
                    'mentions': result.post_data.content.mentions if result.success and result.post_data.content else [],
                } if result.success and result.post_data.content else None,
                'stats': {
                    'likes': result.post_data.stats.likes if result.success and result.post_data.stats else '0',
                    'comments': result.post_data.stats.comments if result.success and result.post_data.stats else '0',
                    'shares': result.post_data.stats.shares if result.success and result.post_data.stats else '0',
                } if result.success and result.post_data.stats else None,
                'media': {
                    'total_found': result.post_data.media_count if result.success else 0,
                    'downloaded': result.media_downloaded,
                    'failed': result.media_failed,
                    'total_size_mb': round(result.total_download_size / 1024 / 1024, 2)
                },
                'files': {
                    'local_folder': result.post_data.local_folder if result.success else None,
                    'content_file': result.post_data.content_file if result.success else None,
                    'metadata_file': result.post_data.metadata_file if result.success else None,
                },
                'processing_time': result.processing_time,
                'error_message': result.error_message
            })
        
        json_file = output_dir / f"premium_results_{int(timestamp)}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        # CSV format (summary)
        import csv
        csv_file = output_dir / f"premium_summary_{int(timestamp)}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['url', 'success', 'user_name', 'post_type', 'likes', 'comments', 'shares', 
                         'content_words', 'media_found', 'media_downloaded', 'download_size_mb', 
                         'local_folder', 'processing_time', 'error_message']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in results:
                writer.writerow({
                    'url': result.url,
                    'success': result.success,
                    'user_name': result.post_data.user_name if result.success else '',
                    'post_type': result.post_data.post_type.value if result.success and result.post_data.post_type else '',
                    'likes': result.post_data.stats.likes if result.success and result.post_data.stats else '0',
                    'comments': result.post_data.stats.comments if result.success and result.post_data.stats else '0',
                    'shares': result.post_data.stats.shares if result.success and result.post_data.stats else '0',
                    'content_words': result.post_data.content.word_count if result.success and result.post_data.content else 0,
                    'media_found': result.post_data.media_count if result.success else 0,
                    'media_downloaded': result.media_downloaded,
                    'download_size_mb': round(result.total_download_size / 1024 / 1024, 2),
                    'local_folder': result.post_data.local_folder if result.success else '',
                    'processing_time': round(result.processing_time, 2),
                    'error_message': result.error_message or ''
                })
        
        # TXT format (readable)
        txt_file = output_dir / f"premium_readable_{int(timestamp)}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write("ðŸš€ FACEBOOK SCRAPER PREMIUM - Káº¾T QUáº¢\n")
            f.write("=" * 80 + "\n\n")
            
            for i, result in enumerate(results, 1):
                f.write(f"ðŸ“ POST #{i}\n")
                f.write("-" * 40 + "\n")
                f.write(f"URL: {result.url}\n")
                
                if result.success:
                    data = result.post_data
                    f.write(f"âœ“ ThÃ nh cÃ´ng\n")
                    f.write(f"ðŸ‘¤ NgÆ°á»i Ä‘Äƒng: {data.user_name or 'Unknown'}\n")
                    f.write(f"ðŸ“Š Loáº¡i bÃ i viáº¿t: {data.post_type.value if data.post_type else 'unknown'}\n")
                    
                    if data.content:
                        f.write(f"ðŸ“ Ná»™i dung ({data.content.word_count} tá»«):\n")
                        f.write(f"{data.content.full_text[:200]}{'...' if len(data.content.full_text) > 200 else ''}\n")
                        if data.content.hashtags:
                            f.write(f"ðŸ·ï¸  Hashtags: {', '.join(data.content.hashtags)}\n")
                        if data.content.mentions:
                            f.write(f"ðŸ‘¥ Mentions: {', '.join(data.content.mentions)}\n")
                    
                    if data.stats:
                        f.write(f"â¤ï¸  {data.stats.likes} likes, ðŸ’¬ {data.stats.comments} comments, ðŸ”„ {data.stats.shares} shares\n")
                    
                    if data.media_count > 0:
                        f.write(f"ðŸ–¼ï¸  Media: {data.media_count} tÃ¬m tháº¥y, {result.media_downloaded} Ä‘Ã£ táº£i\n")
                        if result.total_download_size > 0:
                            f.write(f"ðŸ’¾ Dung lÆ°á»£ng: {result.total_download_size/1024/1024:.1f} MB\n")
                        if data.local_folder:
                            f.write(f"ðŸ“ ThÆ° má»¥c: {data.local_folder}\n")
                    
                    f.write(f"â±ï¸  Thá»i gian xá»­ lÃ½: {result.processing_time:.2f}s\n")
                else:
                    f.write(f"âŒ Tháº¥t báº¡i: {result.error_message}\n")
                
                f.write("\n")
        
        print(f"âœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u:")
        print(f"   â€¢ JSON (chi tiáº¿t): {json_file}")
        print(f"   â€¢ CSV (tÃ³m táº¯t): {csv_file}")
        print(f"   â€¢ TXT (dá»… Ä‘á»c): {txt_file}")
        
        if download_media and any(r.post_data.local_folder for r in results if r.success):
            print(f"   â€¢ Media files: downloads/ folder")
        
        # Show failed URLs if any
        if failed > 0:
            print()
            print("âŒ CÃC URLs THáº¤T Báº I:")
            for result in results:
                if not result.success:
                    print(f"   â€¢ {result.url}: {result.error_message}")
        
        print()
        print("ðŸŽ‰ HoÃ n thÃ nh Premium Scraping!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  NgÆ°á»i dÃ¹ng dá»«ng scraping")
    except Exception as e:
        print(f"\nâŒ Lá»—i trong quÃ¡ trÃ¬nh scraping: {e}")
        logging.error(f"Unexpected error: {e}")
    finally:
        # Cleanup
        print("\nðŸ§¹ Äang dá»n dáº¹p...")
        scraper.shutdown()
        print("âœ… HoÃ n táº¥t!")

def print_usage():
    """Print usage information"""
    print("ðŸš€ FACEBOOK SCRAPER PREMIUM - COMMAND LINE")
    print()
    print("Usage:")
    print("  python run_premium.py [workers] [extract_content] [download_media] [media_quality]")
    print()
    print("Arguments:")
    print("  workers         : Sá»‘ workers (1-10, máº·c Ä‘á»‹nh: 3)")
    print("  extract_content : true/false (máº·c Ä‘á»‹nh: true)")
    print("  download_media  : true/false (máº·c Ä‘á»‹nh: true)")
    print("  media_quality   : low/medium/high (máº·c Ä‘á»‹nh: high)")
    print()
    print("Examples:")
    print("  python run_premium.py                    # Máº·c Ä‘á»‹nh")
    print("  python run_premium.py 5                  # 5 workers")
    print("  python run_premium.py 3 true false       # KhÃ´ng táº£i media")
    print("  python run_premium.py 2 true true medium # Cháº¥t lÆ°á»£ng trung bÃ¬nh")
    print()
    print("Requirements:")
    print("  â€¢ File links.txt chá»©a cÃ¡c Facebook URLs")
    print("  â€¢ Má»—i URL trÃªn má»™t dÃ²ng riÃªng")
    print("  â€¢ Chrome browser Ä‘Ã£ cÃ i Ä‘áº·t")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help', 'help']:
        print_usage()
    else:
        asyncio.run(main()) 